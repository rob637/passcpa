ID,Title,Avatar,Voice,Duration,Type,Script
CISA-D2-SC-02,Scenario: IT Governance Maturity Assessment,Sarah_sitting_20240617,en-US-AriaNeural,14,scenario,"The CEO asks you to assess the maturity of IT governance in the organization. She wants to know where they stand and what they should improve. Here's how you approach it. <break time='1s'/> You start by establishing criteria. What does good IT governance look like? You reference frameworks like COBIT. Key areas include strategic alignment, value delivery, risk management, resource management, and performance measurement. <break time='0.5s'/> You'll use a maturity model. Level zero means nonexistent. Level one means initial or ad hoc. Level two means repeatable but intuitive. Level three means defined processes. Level four means managed and measurable. Level five means optimized. <break time='1s'/> You gather evidence across governance domains. <break time='0.8s'/> For strategic alignment, you look for: Is there an IT strategy? Does it align with business strategy? Does IT leadership participate in business planning? You find there's an IT strategy document, but it's three years old and doesn't reference current business initiatives. You rate this as level two. <break time='0.8s'/> For value delivery, you look for: Are investments evaluated consistently? Is value measured? You find projects are approved ad hoc without consistent criteria. Benefits are rarely tracked after implementation. You rate this as level one. <break time='0.8s'/> For risk management, you look for: Is IT risk in the enterprise risk framework? Are risks assessed regularly? You find IT risk is assessed annually and reported to the board, but risk appetite isn't formally defined. You rate this as level three. <break time='0.8s'/> For resource management, you look for: Is there capacity planning? Are skills managed? You find budget is managed, but human resource planning is reactive. You rate this as level two. <break time='0.8s'/> For performance measurement, you look for: Are there defined metrics? Are they monitored and reported? You find basic operational metrics exist but strategic outcome metrics don't. You rate this as level two. <break time='1s'/> You present findings: Overall governance maturity is level two, repeatable but intuitive. Strengths are in risk management. Weaknesses are in value delivery and strategic alignment. <break time='0.5s'/> Recommendations: Update the IT strategy with explicit business linkage. Implement consistent project evaluation criteria. Define and track benefit realization. Establish strategic performance metrics. <break time='0.5s'/> You've provided the CEO with a clear picture and a roadmap for improvement."
CISA-D3-SC-02,Scenario: System Development Audit,Wayne_20240711_1,en-US-GuyNeural,14,scenario,"You're auditing a critical system development project six months before planned go-live. The project will replace the core billing system. What are you looking for? <break time='1s'/> You start with project documentation. <break time='0.5s'/> You review the project charter, which exists and shows proper authorization. <break time='0.5s'/> You review the requirements document. It's comprehensive, but you notice many requirements are marked TBD even though the project is past the requirements phase. Business users signed off, but the sign-off date was before many requirements were added. <break time='0.5s'/> The design document traces to requirements. Good. But some requirements don't have corresponding design elements. <break time='0.5s'/> The project plan shows aggressive timelines. Testing is scheduled for four weeks. The project manager says that's standard for projects this size, but you know the system is highly complex. <break time='1s'/> You interview stakeholders. <break time='0.5s'/> The project manager is confident but stressed. He mentions the deadline is firm because of a regulatory requirement. <break time='0.5s'/> The development lead says they're coding as fast as they can, but requirements keep changing. Change control exists but doesn't seem to slow changes down. <break time='0.5s'/> The testing lead is worried. She doesn't have enough testers, and she hasn't seen final requirements to develop test cases. <break time='0.5s'/> Business users say they're too busy with daily operations to participate much in the project. <break time='1s'/> You identify risks and control weaknesses. <break time='0.5s'/> Requirements instability this late suggests scope isn't controlled. <break time='0.5s'/> Testing appears inadequate given system complexity and timeline. <break time='0.5s'/> User involvement is insufficient, creating UAT risk. <break time='0.5s'/> The aggressive timeline with a firm deadline creates pressure to cut corners. <break time='1s'/> You draft findings. You don't say the project will fail. You identify specific risks and recommend mitigating actions: Stabilize requirements and implement stronger change control. Increase testing resources and duration. Require dedicated business user participation. <break time='0.5s'/> Your audit adds value by identifying problems while there's still time to address them."
CISA-D4-SC-02,Scenario: Assessing DR Readiness,josh_lite3_20230714,en-US-ChristopherNeural,14,scenario,"The audit committee wants assurance that the organization can recover from a disaster. You're conducting a disaster recovery readiness assessment. <break time='1s'/> You start with documentation. <break time='0.5s'/> You request the Business Impact Analysis. It exists but was last updated three years ago. New systems have been implemented since then. <break time='0.5s'/> You request the disaster recovery plan. It's a thick document. Section by section, it appears comprehensive: recovery procedures, contact lists, resource requirements. But the version control shows no updates in eighteen months. <break time='0.5s'/> You request test results. The last test was a tabletop exercise fifteen months ago. Results showed several gaps. Follow-up actions were assigned but you see no evidence they were completed. <break time='1s'/> You examine recovery capabilities. <break time='0.5s'/> The organization has a contract with a DR provider for a warm site. You verify the contract is current. <break time='0.5s'/> Cloud systems are in a single availability zone. No cross-region replication exists. <break time='0.5s'/> Backup tapes are stored off-site, but restoration testing hasn't been performed in two years. <break time='1s'/> You conduct interviews. <break time='0.5s'/> IT management says DR is a priority but other initiatives took precedence. <break time='0.5s'/> The DR coordinator role is vacant. The previous coordinator left six months ago. <break time='0.5s'/> Staff are unsure of their recovery responsibilities. They think there's a plan somewhere but haven't seen it. <break time='1s'/> Your assessment is concerning. <break time='0.5s'/> The organization has DR documentation, but it's outdated. <break time='0.5s'/> Recovery capabilities haven't been verified. <break time='0.5s'/> Testing is insufficient and follow-through is lacking. <break time='0.5s'/> No one is driving the program since the coordinator left. <break time='1s'/> Recommendations: Assign DR program ownership immediately. Update the BIA to reflect current systems. Revise the DR plan to address BIA findings. Conduct a recovery test within ninety days. Establish an annual testing schedule. <break time='0.5s'/> You're providing the audit committee with an honest assessment: on paper it looks adequate, but dig deeper and there are gaps."
CISA-D5-SC-02,Scenario: Security Incident Response Assessment,Daisy-iทvestment_V2,en-US-JennyNeural,14,scenario,"Following a ransomware attack at a competitor, management wants assurance that the organization can respond effectively to a security incident. You're assessing incident response readiness. <break time='1s'/> You request the incident response plan. A fifty-page document arrives. It defines incident categories, response procedures, roles and responsibilities, and communication protocols. On paper, it looks thorough. <break time='1s'/> You examine specific elements. <break time='0.5s'/> The plan defines an Incident Response Team with named members. You verify the team exists and members know their roles. Most do, though two positions have changed without updating the plan. <break time='0.5s'/> Contact information is in an appendix. Some numbers are outdated. The emergency contact for the CEO is a number that's been disconnected for a year. <break time='0.5s'/> Communication templates exist for notifying customers, regulators, and media. Legal has reviewed them. Good. <break time='0.5s'/> The plan references forensics capabilities, but the organization doesn't have them internally. The plan says to call a forensics firm, but no contract exists. During an incident, you'd be negotiating terms while under attack. <break time='1s'/> You review past incidents. <break time='0.5s'/> There have been several minor incidents in the past year. Incident tickets exist. Response varied. Some incidents followed the plan. Others were handled informally. One significant incident was escalated appropriately and documented well. <break time='1s'/> You ask about testing. <break time='0.5s'/> The organization conducted a tabletop exercise two years ago. No exercises since. The exercise identified communication gaps that remain unaddressed. <break time='1s'/> Your findings: <break time='0.5s'/> A formal plan exists, which is positive. <break time='0.5s'/> The plan needs updating for personnel and contact changes. <break time='0.5s'/> Forensics capability should be formalized through retainer agreement. <break time='0.5s'/> Exercise frequency should increase to annual. <break time='0.5s'/> Plan should be socialized more broadly so all staff know basic procedures. <break time='1s'/> You conclude: Incident response capability is developing but not mature. With specific improvements, the organization can significantly strengthen its readiness."
CISA-D1-MA-02,Ten Audit Principles Quick Review,Marcus_20231218,en-US-DavisNeural,10,memory_aid,"Let's nail down the core principles auditors live by. Here's a rapid-fire review. <break time='1s'/> Principle one: Independence. Free from conflicts that compromise judgment. You audit, you don't implement. <break time='0.8s'/> Principle two: Objectivity. Impartial assessment based on evidence, not bias or assumption. <break time='0.8s'/> Principle three: Evidence-based conclusions. Every finding supported by sufficient, reliable evidence. <break time='0.8s'/> Principle four: Risk-based planning. Focus resources on highest risks. You can't audit everything. <break time='0.8s'/> Principle five: Professional skepticism. Question, verify, corroborate. Trust but verify. <break time='0.8s'/> Principle six: Due professional care. Work with the skill expected of a reasonably prudent auditor. <break time='0.8s'/> Principle seven: Confidentiality. Protect information obtained during the audit. <break time='0.8s'/> Principle eight: Fact-based reporting. Report what you found, not what you assume. <break time='0.8s'/> Principle nine: Value-added recommendations. Don't just find problems, suggest solutions. <break time='0.8s'/> Principle ten: Continuous improvement. Learn from each audit. Refine your approach. <break time='1s'/> On exam day, when you're unsure between two answers, ask yourself: Which answer best reflects these principles? <break time='0.5s'/> The auditor assesses, doesn't implement. The auditor relies on evidence, not assumptions. The auditor focuses on risk. The auditor maintains independence. <break time='0.5s'/> These principles guide you to the right answer."
CISA-D5-MA-02,Security Acronyms Rapid Fire,Daisy-iทvestment_V2,en-US-JennyNeural,8,memory_aid,"Security acronyms appear constantly on the exam. Let's lock them in. <break time='0.8s'/> CIA: Confidentiality, Integrity, Availability. The three pillars of security. <break time='0.5s'/> IDS: Intrusion Detection System. Detects and alerts. <break time='0.5s'/> IPS: Intrusion Prevention System. Detects and blocks. <break time='0.5s'/> VPN: Virtual Private Network. Encrypted tunnel over untrusted networks. <break time='0.5s'/> DMZ: Demilitarized Zone. Network segment between trusted and untrusted. <break time='0.5s'/> SIEM: Security Information and Event Management. Collects logs, correlates events, generates alerts. <break time='0.5s'/> DLP: Data Loss Prevention. Monitors for unauthorized data exfiltration. <break time='0.5s'/> MFA: Multi-Factor Authentication. Something you know, have, or are. <break time='0.5s'/> PKI: Public Key Infrastructure. Manages digital certificates and keys. <break time='0.5s'/> AES: Advanced Encryption Standard. Current symmetric encryption standard. <break time='0.5s'/> RSA: Rivest Shamir Adleman. Common asymmetric algorithm. <break time='0.5s'/> SHA: Secure Hash Algorithm. Creates message digests. <break time='0.5s'/> SSL and TLS: Secure Sockets Layer and Transport Layer Security. Encrypt web traffic. <break time='0.5s'/> DAC: Discretionary Access Control. Owner controlled. <break time='0.5s'/> MAC: Mandatory Access Control. Clearance and classification. <break time='0.5s'/> RBAC: Role-Based Access Control. Based on job function. <break time='0.8s'/> Rapid fire complete. These appear on the exam constantly. Know them cold."
CISA-D4-MA-02,Recovery Objectives RTOs and RPOs,josh_lite3_20230714,en-US-ChristopherNeural,8,memory_aid,"Recovery objectives appear constantly. Let's cement them. <break time='0.8s'/> RTO: Recovery Time Objective. How fast must you recover? <break time='0.5s'/> Think T for Time. RTO equals Time to recover. If RTO is four hours, the system must be operational within four hours. <break time='0.8s'/> RPO: Recovery Point Objective. How much data can you lose? <break time='0.5s'/> Think P for Point in time. RPO is how far back you go. If RPO is one hour, you can lose up to one hour of data. Backups must occur at least every hour. <break time='0.8s'/> MTD: Maximum Tolerable Downtime. The absolute limit. <break time='0.5s'/> Think M for Maximum. Beyond MTD, the business fails. MTD must be greater than RTO, otherwise your recovery target is impossible. <break time='0.8s'/> Quick math example: <break time='0.5s'/> Business says payroll must be recovered within 24 hours. That's RTO. <break time='0.5s'/> Business says they can recreate one day of transactions. That's RPO. <break time='0.5s'/> Business says without payroll for more than 48 hours, employees leave. That's MTD. <break time='0.8s'/> Recovery capabilities must meet objectives. <break time='0.5s'/> If RTO is four hours, you need a hot site, not a cold site. <break time='0.5s'/> If RPO is one hour, you need hourly backups or real-time replication. <break time='0.8s'/> Exam pattern: Scenario gives recovery requirements. You identify whether capabilities meet them. If RTO is four hours but recovery takes twelve, that's a gap."
CISA-D2-MA-02,Governance Frameworks at a Glance,Sarah_sitting_20240617,en-US-AriaNeural,8,memory_aid,"Governance frameworks can blur together. Let's distinguish them clearly. <break time='0.8s'/> COBIT: Control Objectives for Information and Related Technology. ISACA's framework. <break time='0.5s'/> Purpose: IT governance and management. It answers how IT should be governed to support enterprise objectives. <break time='0.5s'/> Know it for: Governance structure, management objectives, control objectives. <break time='0.8s'/> ITIL: Information Technology Infrastructure Library. IT service management framework. <break time='0.5s'/> Purpose: Delivering IT services effectively. It answers how IT should operate day to day. <break time='0.5s'/> Know it for: Service desk, incident management, problem management, change management. <break time='0.8s'/> ISO 27001: International security standard. <break time='0.5s'/> Purpose: Information security management system. It defines requirements for establishing, implementing, and improving security. <break time='0.5s'/> Know it for: Security controls, certification, risk assessment. <break time='0.8s'/> COSO: Committee of Sponsoring Organizations. Internal control framework. <break time='0.5s'/> Purpose: Enterprise risk management and internal control. Broader than IT. <break time='0.5s'/> Know it for: Control environment, risk assessment, control activities, monitoring. <break time='0.8s'/> Quick associations: <break time='0.5s'/> Governance question: Think COBIT. <break time='0.5s'/> Operations question: Think ITIL. <break time='0.5s'/> Security question: Think ISO 27001. <break time='0.5s'/> Internal control question: Think COSO. <break time='0.5s'/> Master these and framework questions become straightforward."
CISA-D3-MA-02,Testing Types Quick Review,Wayne_20240711_1,en-US-GuyNeural,8,memory_aid,"Testing types show up constantly. Let's lock them in. <break time='0.8s'/> Unit testing: Smallest piece. Individual module or function. Done by developers. <break time='0.5s'/> Integration testing: Modules combined. Testing interfaces between components. Done by developers or QA. <break time='0.5s'/> System testing: Complete system. End-to-end functionality. Done by QA team. <break time='0.5s'/> UAT: User acceptance testing. Business users verify it meets their needs. Done by business. <break time='0.5s'/> Regression testing: Verify changes don't break existing functionality. Re-run previous tests. <break time='0.8s'/> Specialized testing types: <break time='0.5s'/> Performance testing: Does it handle the load? Response times under stress. <break time='0.5s'/> Security testing: Can it be compromised? Penetration testing, vulnerability scanning. <break time='0.5s'/> Alpha testing: Internal testing before external. <break time='0.5s'/> Beta testing: External testing before general release. <break time='0.8s'/> Key pattern: Testing progresses from small to large, from developers to users. Unit, integration, system, UAT. <break time='0.5s'/> Exam application: If the question asks what type of testing the business performs, it's UAT. If the question asks what testing verifies interfaces, it's integration. If the question asks what testing catches defects introduced by changes, it's regression."
