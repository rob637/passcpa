/**
 * CPA ISC Section - Questions Depth Batch 9
 * Blueprint Area: ISC-I (Information Systems)
 * Topics: IT Operations, Backup/Recovery, Capacity Planning, Mobile Security, IoT
 */

import { Question } from '../../../types';

export const ISC_QUESTIONS_DEPTH_9: Question[] = [
  {
    id: 'isc-d9-001',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-continuity',
    topic: 'Business Continuity',
    subtopic: 'Backup Strategies',
    difficulty: 'medium',
    skillLevel: 'Application',
    question: 'A differential backup captures:',
    options: [
      'All data on the system every time',
      'All data that has changed since the last FULL backup — each differential backup grows larger over time until the next full backup, and recovery requires only the last full backup plus the latest differential',
      'Only data changed since the last backup of any type',
      'Only system configuration files',
    ],
    correctAnswer: 1,
    explanation: 'Backup types: (1) Full backup — copies ALL data; foundation for other backup types; slowest to create but fastest/simplest restore, (2) Differential backup — copies data changed since last FULL backup; grows larger each day; restore = full + latest differential (2 backups), (3) Incremental backup — copies data changed since last backup OF ANY TYPE; smallest daily backup; restore = full + all incrementals in sequence (many backups, more complex recovery). Example weekly schedule: Full (Sunday), then daily incrementals or differentials. Trade-offs: differential = faster recovery (2 sets) but larger daily backups; incremental = smaller daily backups but slower recovery (multiple sets). Additional backup methods: (1) Continuous data protection (CDP) — real-time replication of every data change, (2) Snapshots — point-in-time copy of data, (3) 3-2-1 rule — 3 copies, 2 different media types, 1 offsite. CPA relevance: backup integrity supports business continuity for financial data.',
    reference: 'NIST SP 800-34; ISACA',
  },
  {
    id: 'isc-d9-002',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-continuity',
    topic: 'Business Continuity',
    subtopic: 'Recovery Point and Time Objectives',
    difficulty: 'medium',
    skillLevel: 'Remembering and Understanding',
    question: 'Recovery Point Objective (RPO) defines:',
    options: [
      'How quickly systems must be restored',
      'The maximum acceptable amount of data loss measured in time — for example, an RPO of 4 hours means the organization can tolerate losing up to 4 hours of data',
      'The cost of disaster recovery',
      'The number of backup copies required',
    ],
    correctAnswer: 1,
    explanation: 'Disaster recovery metrics: (1) RPO (Recovery Point Objective) — maximum tolerable data loss (in time). RPO = 0 → no data loss acceptable (requires synchronous replication/CDP). RPO = 4 hours → can lose up to 4 hours of data (backup every 4 hours is sufficient). (2) RTO (Recovery Time Objective) — maximum tolerable downtime. RTO = 0 → no downtime acceptable (requires hot standby/active-active). RTO = 24 hours → system must be restored within 24 hours. (3) MTPD (Maximum Tolerable Period of Disruption) — business-defined maximum downtime before unacceptable consequences. (4) WRT (Work Recovery Time) — time to verify restored systems and bring them to operational state (RTO + WRT ≤ MTPD). Determining RPO/RTO: Business Impact Analysis (BIA) identifies critical systems and their tolerance for data loss/downtime. Cost consideration: lower RPO/RTO = higher cost (more sophisticated replication, redundancy). CPA relevance: RPO/RTO for financial systems determines whether GL data, transaction records, and reporting systems meet financial reporting deadlines.',
    reference: 'NIST SP 800-34; ISO 22301',
  },
  {
    id: 'isc-d9-003',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-continuity',
    topic: 'Business Continuity',
    subtopic: 'DR Site Types',
    difficulty: 'easy',
    skillLevel: 'Remembering and Understanding',
    question: 'A hot site disaster recovery facility provides:',
    options: [
      'Only building space without any equipment',
      'A fully equipped, operational facility with current hardware, software, data (near real-time replication), and network connectivity — ready for immediate failover with minimal downtime',
      'Equipment but no data',
      'Data backups stored in a vault only',
    ],
    correctAnswer: 1,
    explanation: 'DR site types (from most to least ready): (1) Hot site — fully equipped with current hardware, software, data (replicated), and network. Ready for immediate or near-immediate failover (minutes to hours). Most expensive, lowest RTO. (2) Warm site — has hardware and network but data is NOT current (restored from backups). Takes hours to days to become operational. Lower cost than hot site, moderate RTO. (3) Cold site — only provides building space (power, HVAC, some connectivity). No hardware or data pre-installed. Equipment must be procured and installed, data restored from offsite backups. Takes days to weeks. Lowest cost, highest RTO. (4) Mobile site — portable facility (trailer/container) that can be deployed to a location. (5) Cloud-based DR — IaaS-based recovery (DRaaS — Disaster Recovery as a Service). Trade-off: cost vs. recovery time. CPA relevance: financial systems typically require hot or warm sites to meet financial reporting deadlines.',
    reference: 'NIST SP 800-34; ISO 22301',
  },
  {
    id: 'isc-d9-004',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-systems',
    topic: 'Enterprise Systems',
    subtopic: 'Containerization',
    difficulty: 'hard',
    skillLevel: 'Analysis',
    question: 'Container technology (e.g., Docker, Kubernetes) differs from traditional virtual machines (VMs) in that:',
    options: [
      'Containers include a full operating system in each instance',
      'Containers share the host OS kernel and package only the application and its dependencies — making them lighter, faster to start, and more portable, but requiring different security considerations than VMs',
      'VMs are always faster than containers',
      'Containers cannot run in the cloud',
    ],
    correctAnswer: 1,
    explanation: 'VM vs Container: VM — includes full guest OS + applications on hypervisor; stronger isolation (hardware-level), heavier (GB), slower startup (minutes). Container — shares host OS kernel; includes only application + dependencies; lighter (MB), faster startup (seconds), more portable. Kubernetes: orchestration platform for managing containers at scale (deployment, scaling, networking, health monitoring). Security considerations: (1) Containers share the kernel — kernel vulnerability affects all containers, (2) Image security — base images must be from trusted sources, scanned for vulnerabilities, (3) Container escape — similar to VM escape but different attack surface, (4) Immutable infrastructure — containers are typically replaced, not patched. Audit implications: (1) Evaluate container image scanning and management, (2) Review Kubernetes RBAC and network policies, (3) Assess secrets management (credentials, API keys in containers), (4) Verify logging and monitoring of container activities.',
    reference: 'NIST SP 800-190; CIS Kubernetes',
  },
  {
    id: 'isc-d9-005',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-network',
    topic: 'Network Architecture',
    subtopic: 'Zero Trust Architecture',
    difficulty: 'hard',
    skillLevel: 'Analysis',
    question: 'Zero Trust Architecture (ZTA) is based on the principle of:',
    options: [
      'Trusting all internal network traffic',
      '"Never trust, always verify" — every access request is authenticated, authorized, and continuously validated regardless of where it originates (inside or outside the network), eliminating implicit trust based on network location',
      'Disabling all firewalls',
      'Allowing all encrypted traffic without inspection',
    ],
    correctAnswer: 1,
    explanation: 'Zero Trust (NIST SP 800-207): Core principles: (1) Never trust, always verify — no implicit trust based on network location, IP address, or device ownership, (2) Assume breach — design controls assuming the network is already compromised, (3) Verify explicitly — authenticate and authorize every request based on all available data points (identity, device health, location, behavior), (4) Least privilege access — grant minimum necessary access for minimum time, (5) Micro-segmentation — fine-grained network/workload segmentation, (6) Continuous monitoring — continuously evaluate risk and adjust access in real-time. Key technologies: (1) Identity and Access Management (IAM) with MFA, (2) Software-Defined Perimeter (SDP), (3) Micro-segmentation, (4) Endpoint Detection and Response (EDR), (5) Security Information and Event Management (SIEM). CPA relevance: ZTA is increasingly adopted for financial systems; auditors should understand ZTA when evaluating access controls.',
    reference: 'NIST SP 800-207; Forrester ZTX',
  },
  {
    id: 'isc-d9-006',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-emerging',
    topic: 'Emerging Technologies',
    subtopic: 'Internet of Things (IoT)',
    difficulty: 'medium',
    skillLevel: 'Application',
    question: 'The Internet of Things (IoT) introduces security risks to organizations because:',
    options: [
      'IoT devices are always more secure than computers',
      'Many IoT devices have limited computing resources, lack built-in security features (no encryption, default passwords, infrequent patching), and expand the organization\'s attack surface significantly',
      'IoT devices cannot connect to networks',
      'IoT devices are only used in homes',
    ],
    correctAnswer: 1,
    explanation: 'IoT security challenges: (1) Limited resources — many IoT devices have constrained CPU/memory, making full encryption and security updates difficult, (2) Default credentials — shipped with default or weak passwords that are often not changed, (3) Infrequent patching — many IoT devices lack automatic update mechanisms or vendor support ends early, (4) Large attack surface — organizations may have thousands of IoT devices (cameras, HVAC, access control, printers, sensors), each a potential entry point, (5) Lack of visibility — many organizations don\'t have a complete inventory of IoT devices on their network, (6) Insecure protocols — some IoT devices use unencrypted communication protocols. Controls: (1) Network segmentation (isolate IoT on separate VLAN), (2) Device inventory and management, (3) Change default credentials, (4) Disable unnecessary services/ports, (5) Monitor IoT network traffic for anomalies, (6) Include IoT in vulnerability management program. CPA relevance: IoT devices in financial environments (access controls, surveillance) must be secured.',
    reference: 'NIST SP 800-183; OWASP IoT',
  },
  {
    id: 'isc-d9-007',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-controls',
    topic: 'Application Controls',
    subtopic: 'Automated Three-Way Matching',
    difficulty: 'medium',
    skillLevel: 'Application',
    question: 'Automated three-way matching in a procurement system compares:',
    options: [
      'Three different financial statements',
      'The purchase order, the receiving report, and the vendor invoice — ensuring that what was ordered matches what was received and what is being billed before authorizing payment',
      'Three backup copies of data',
      'Three audit workpapers',
    ],
    correctAnswer: 1,
    explanation: 'Three-way matching: automated control in accounts payable: (1) Purchase Order (PO) — what was ordered (items, quantities, prices, terms), (2) Receiving Report — what was actually received (items, quantities, condition), (3) Vendor Invoice — what the vendor is billing. The system automatically compares: quantities (PO vs received vs invoiced), prices (PO vs invoiced), terms (PO vs invoice). Tolerance: organizations set tolerance thresholds (e.g., ±$5 or ±1%) — matches within tolerance are auto-approved for payment; exceptions require manual review. Benefits: (1) Prevents payment for goods not ordered or not received, (2) Detects billing errors and fraud (inflated quantities, prices), (3) Reduces manual effort and processing time. Extensions: (1) Two-way match (PO vs invoice — for services without receiving), (2) Four-way match (adds inspection report). Audit test: verify matching parameters are appropriately set, test a sample of exceptions for proper resolution.',
    reference: 'AICPA; COSO Procurement Controls',
  },
  {
    id: 'isc-d9-008',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-sdlc',
    topic: 'Systems Development',
    subtopic: 'Data Conversion and Migration',
    difficulty: 'hard',
    skillLevel: 'Analysis',
    question: 'During a system migration, the MOST critical control for ensuring data integrity is:',
    options: [
      'Using the newest hardware',
      'Performing comprehensive data validation and reconciliation between the source and target systems — verifying record counts, control totals, and data completeness before, during, and after migration',
      'Training users on the new interface',
      'Archiving old data immediately',
    ],
    correctAnswer: 1,
    explanation: 'Data migration controls: Pre-migration: (1) Data cleansing — identify and resolve data quality issues in source system before migration (duplicates, incomplete records, invalid values), (2) Mapping documentation — detailed field-by-field mapping from source to target, (3) Control totals — record counts, hash totals, financial totals calculated in source system. During migration: (4) Validation rules — automated checks during transformation (format, completeness, referential integrity), (5) Error handling — capture and log rejected/failed records for review. Post-migration: (6) Reconciliation — compare record counts, financial totals, and key data elements between source and target, (7) Parallel processing — run both systems simultaneously for a period to compare outputs, (8) User verification — business users validate data accuracy in the new system, (9) Rollback plan — ability to revert to source system if critical issues found. Audit approach: verify reconciliation procedures, test a sample of migrated records for accuracy, review exception reports.',
    reference: 'ISACA; COBIT BAI',
  },
  {
    id: 'isc-d9-009',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-it-governance',
    topic: 'IT Governance',
    subtopic: 'Service Level Agreements',
    difficulty: 'easy',
    skillLevel: 'Remembering and Understanding',
    question: 'A Service Level Agreement (SLA) between an IT service provider and a customer should specify:',
    options: [
      'Only the total contract price',
      'Measurable service metrics including availability targets, response times, performance benchmarks, escalation procedures, and remedies/penalties for non-compliance',
      'The provider\'s internal staffing decisions',
      'The customer\'s marketing strategy',
    ],
    correctAnswer: 1,
    explanation: 'SLA key components: (1) Service description — what services are covered (scope), (2) Availability — uptime targets (e.g., 99.9% = 8.76 hours downtime/year), (3) Performance — response time, throughput, transaction processing speed, (4) Support — response time by severity (Critical: 15 min, High: 1 hour, Medium: 4 hours), (5) Escalation procedures — who is contacted at each level, (6) Maintenance windows — scheduled downtime for updates, (7) Reporting — regular performance reports and service reviews, (8) Penalties/credits — remedies if SLA is not met (service credits, termination rights), (9) Exclusions — what is NOT covered. SLA monitoring: dashboards tracking KPIs, regular review meetings. CPA relevance: (1) SLAs for financial system hosting define acceptable system reliability, (2) SLA breaches may affect financial reporting timelines, (3) Auditors review SLAs and compliance reports for outsourced services.',
    reference: 'ITIL 4; ISACA',
  },
  {
    id: 'isc-d9-010',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-systems',
    topic: 'Enterprise Systems',
    subtopic: 'Point-of-Sale (POS) Systems',
    difficulty: 'medium',
    skillLevel: 'Application',
    question: 'Point-of-Sale (POS) systems that process credit card transactions must comply with:',
    options: [
      'Only local building codes',
      'PCI DSS (Payment Card Industry Data Security Standard) — which requires controls over cardholder data protection, network security, access controls, monitoring, and regular testing to prevent credit card data breaches',
      'FDA food safety regulations',
      'OSHA workplace safety standards only',
    ],
    correctAnswer: 1,
    explanation: 'PCI DSS (current version 4.0): 12 requirements in 6 categories: Build and Maintain a Secure Network: (1) Install and maintain network security controls (firewalls), (2) Apply secure configurations (change defaults). Protect Account Data: (3) Protect stored account data (encryption, masking, truncation), (4) Encrypt cardholder data in transit (TLS). Maintain a Vulnerability Management Program: (5) Protect from malicious software (antivirus), (6) Develop and maintain secure systems (patching, secure SDLC). Implement Strong Access Control: (7) Restrict access by business need, (8) Identify and authenticate users (MFA for admin access), (9) Restrict physical access to cardholder data. Monitor and Test: (10) Log and monitor all access, (11) Test security regularly (vulnerability scans, penetration testing). Maintain an Information Security Policy: (12) Security policy for all personnel. POS-specific: (1) Point-to-point encryption (P2PE), (2) EMV chip readers, (3) Tokenization. Non-compliance: fines, loss of ability to process cards, liability for breaches.',
    reference: 'PCI DSS v4.0',
  },
  {
    id: 'isc-d9-011',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-data-analytics',
    topic: 'Data Analytics',
    subtopic: 'Predictive vs Prescriptive Analytics',
    difficulty: 'medium',
    skillLevel: 'Remembering and Understanding',
    question: 'Prescriptive analytics differs from predictive analytics in that prescriptive analytics:',
    options: [
      'Only looks at historical data',
      'Not only predicts what is likely to happen but also recommends specific actions to optimize outcomes — using optimization algorithms, simulation, and decision models to advise the best course of action',
      'Uses no data at all',
      'Is the same as descriptive analytics',
    ],
    correctAnswer: 1,
    explanation: 'Analytics maturity: (1) Descriptive analytics — "What happened?" Summarizes historical data (reports, dashboards, KPIs). Tools: BI platforms, Excel, SQL. (2) Diagnostic analytics — "Why did it happen?" Root cause analysis, drill-down, data mining. (3) Predictive analytics — "What is likely to happen?" Forecasts future outcomes using statistical models, ML, regression analysis. Examples: revenue forecasting, credit risk scoring, fraud probability. (4) Prescriptive analytics — "What should we do?" Recommends optimal actions using optimization, simulation, and decision models. Examples: optimal pricing strategy, resource allocation, inventory reorder points, best hedging strategy. CPA relevance: (1) Descriptive — financial reporting and variance analysis, (2) Diagnostic — audit analytics (why did a balance change unexpectedly?), (3) Predictive — going concern assessment, allowance estimation, (4) Prescriptive — tax planning optimization, strategic advisory.',
    reference: 'AICPA Data Analytics; Gartner',
  },
  {
    id: 'isc-d9-012',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-controls',
    topic: 'Application Controls',
    subtopic: 'Workflow Automation Controls',
    difficulty: 'medium',
    skillLevel: 'Application',
    question: 'An automated workflow approval system enforces:',
    options: [
      'That all transactions bypass management review',
      'Predefined authorization rules — routing transactions to the appropriate approvers based on type, amount, and organizational hierarchy, ensuring proper authorization before processing and creating an audit trail of all approvals',
      'That approvals are only verbal',
      'That all transactions are approved by the CEO',
    ],
    correctAnswer: 1,
    explanation: 'Workflow automation controls: (1) Routing rules — automatically direct transactions to appropriate approver(s) based on: amount thresholds, transaction type, department, cost center, vendor, (2) Escalation — if an approver doesn\'t act within a defined period, the workflow escalates to a backup approver or manager, (3) Delegation — approvers can temporarily delegate authority (with controls), (4) Segregation of duties — workflow prevents the same person from initiating and approving, (5) Multi-level approval — high-value transactions require multiple approvals (e.g., expenses > $10,000 need director + VP), (6) Audit trail — complete record of: who submitted, who approved, timestamps, any notes/comments, delegation details. Benefits: (1) Consistent enforcement of authorization policies, (2) Faster processing than paper-based approval, (3) Complete audit trail, (4) Reduced risk of unauthorized transactions. Audit test: verify workflow rules match authorization policies, test a sample of transactions for proper routing and approval.',
    reference: 'COSO Internal Control; ISACA',
  },
  {
    id: 'isc-d9-013',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-network',
    topic: 'Network Architecture',
    subtopic: 'Load Balancing and High Availability',
    difficulty: 'medium',
    skillLevel: 'Remembering and Understanding',
    question: 'A load balancer improves system availability by:',
    options: [
      'Backing up data more frequently',
      'Distributing incoming network traffic across multiple servers — if one server fails, the load balancer redirects traffic to remaining healthy servers, preventing a single point of failure',
      'Encrypting all network traffic',
      'Increasing the speed of individual servers',
    ],
    correctAnswer: 1,
    explanation: 'Load balancing: distributes workload across multiple servers to optimize performance and ensure availability. Types: (1) Round-robin — requests distributed sequentially, (2) Least connections — sent to server with fewest active connections, (3) IP hash — client IP determines server (session persistence), (4) Weighted — servers assigned priority based on capacity, (5) Health-check-based — only routes to servers that pass health checks. High availability concepts: (1) Redundancy — duplicate critical components (servers, network connections, power supplies), (2) Failover — automatic switching to backup when primary fails (active-passive or active-active), (3) Clustering — multiple servers working as one logical system, (4) Geographic redundancy — distributed across multiple data centers/regions. Metrics: (1) Uptime target (99.9% = 8.76 hrs/year downtime, 99.99% = 52 min/year), (2) Failover time — seconds to minutes ideally. CPA relevance: high availability of financial systems ensures timely transaction processing and reporting.',
    reference: 'NIST; AWS Well-Architected',
  },
  {
    id: 'isc-d9-014',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-emerging',
    topic: 'Emerging Technologies',
    subtopic: 'Cloud-Native Architecture',
    difficulty: 'hard',
    skillLevel: 'Analysis',
    question: 'Serverless computing (Function as a Service) changes the security model because:',
    options: [
      'There are no servers involved at all',
      'The organization only manages application code and configuration — the cloud provider manages all infrastructure, OS, and runtime security, shifting the security focus to code security, API protections, function-level permissions, and data protection',
      'Security is entirely the cloud provider\'s responsibility',
      'Serverless applications cannot be breached',
    ],
    correctAnswer: 1,
    explanation: 'Serverless security considerations: What the provider manages: servers, OS patching, runtime, scaling, availability. What the customer manages: (1) Application code — must be secure (input validation, injection prevention), (2) Function permissions — each function should have minimum required permissions (least privilege IAM roles), (3) Event triggers — secure the event sources that invoke functions (API Gateway, queue, S3), (4) Dependencies — third-party libraries must be scanned for vulnerabilities, (5) Data — encryption, access controls, privacy compliance, (6) Authentication/authorization — API keys, OAuth, JWT tokens, (7) Logging and monitoring — function-level logging for security events. Unique risks: (1) Short function execution time limits forensic analysis, (2) Third-party dependency risks (supply chain attacks), (3) Cold start behavior may affect availability, (4) Vendor lock-in (functions tightly coupled to provider\'s services). Audit approach: review IAM policies, test API security, verify logging, assess code scanning practices.',
    reference: 'OWASP Serverless Top 10; CSA',
  },
  {
    id: 'isc-d9-015',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-audit',
    topic: 'IT Audit Techniques',
    subtopic: 'Embedded Audit Modules',
    difficulty: 'hard',
    skillLevel: 'Analysis',
    question: 'Embedded audit modules (EAMs) or SCARF (Systems Control and Review File) are audit techniques that:',
    options: [
      'Are installed after the system goes live',
      'Are built into the application during development — they capture selected transactions meeting auditor-defined criteria (exceptions, high-value items, unusual patterns) into a special audit file for later review without disrupting normal processing',
      'Replace all manual audit procedures',
      'Only work with mainframe systems',
    ],
    correctAnswer: 1,
    explanation: 'Embedded Audit Modules (EAMs)/SCARF: (1) Design — audit modules are coded into the application during development (or added as hooks), (2) Criteria — auditor defines capture criteria: transactions exceeding thresholds, unusual patterns, exceptions, SoD violations, (3) Processing — as transactions are processed normally, the EAM evaluates each against criteria and writes qualifying transactions to a separate audit file (SCARF), (4) Review — auditor periodically retrieves and analyzes the SCARF file. Advantages: (1) Continuous monitoring without disrupting operations, (2) Captures transactions in real-time at the point of processing, (3) Can identify issues that might be missed in periodic reviews, (4) Supports continuous auditing. Disadvantages: (1) Must be planned during development (difficult to add later), (2) Requires cooperation between auditors and developers, (3) Performance impact if criteria are too broad, (4) SCARF files must be secured from tampering.',
    reference: 'IIA GTAG; AICPA',
  },
  {
    id: 'isc-d9-016',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-it-governance',
    topic: 'IT Governance',
    subtopic: 'IT Outsourcing Governance',
    difficulty: 'medium',
    skillLevel: 'Application',
    question: 'When governing an outsourced IT relationship, the client organization should:',
    options: [
      'Completely delegate all IT decisions to the vendor',
      'Maintain an informed buyer capability — retaining sufficient internal IT expertise to define requirements, evaluate vendor performance, manage the contract, and assess risks throughout the engagement',
      'Eliminate its internal IT function entirely',
      'Only communicate with the vendor annually',
    ],
    correctAnswer: 1,
    explanation: 'IT outsourcing governance: (1) Informed buyer — retain enough internal IT competence to: define clear requirements and SLAs, evaluate vendor proposals intelligently, manage day-to-day relationship, monitor performance against SLAs, assess vendor risks, plan for contract renewal or transition. (2) Contract management — regular review of: SLA compliance, change orders and scope creep, financial terms and invoicing accuracy, vendor staffing and key personnel changes. (3) Relationship management — regular governance meetings (monthly operational, quarterly strategic), escalation procedures, joint risk management. (4) Exit strategy — plan for transitioning services back in-house or to another vendor: data portability requirements, transition assistance obligations, knowledge transfer. (5) Multi-vendor management — if multiple vendors, ensure clear accountability and coordination. Audit focus: evaluate vendor governance process, review SLA performance reports, assess vendor risk management.',
    reference: 'COBIT APO10; ISACA',
  },
  {
    id: 'isc-d9-017',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-systems',
    topic: 'Enterprise Systems',
    subtopic: 'Middleware and Message Queuing',
    difficulty: 'hard',
    skillLevel: 'Analysis',
    question: 'In enterprise architecture, middleware serves the function of:',
    options: [
      'Storing financial data permanently',
      'Facilitating communication and data exchange between different applications and systems — acting as a bridge that handles message routing, protocol translation, data transformation, and transaction management between disparate systems',
      'Replacing the operating system',
      'Providing antivirus protection',
    ],
    correctAnswer: 1,
    explanation: 'Middleware types: (1) Message-Oriented Middleware (MOM) — asynchronous message queuing (e.g., IBM MQ, RabbitMQ, Apache Kafka). Messages placed in queues for reliable delivery even if receiving system is temporarily unavailable. (2) Transaction Processing Middleware — coordinates distributed transactions across multiple systems (two-phase commit), (3) Application servers — host business logic between client and database (J2EE, .NET), (4) Enterprise Service Bus (ESB) — centralized integration that routes messages between services, applies transformations, and handles protocol conversion, (5) API Gateway — manages REST/SOAP API traffic (routing, authentication, rate limiting). Audit implications: (1) Message integrity — ensure messages are not lost, duplicated, or corrupted, (2) Error handling — failed message processing must be logged and reviewed, (3) Security — messages may contain sensitive data (encryption in transit/at rest), (4) Performance — bottleneck in middleware affects all connected systems, (5) Access controls — who can send/receive via middleware.',
    reference: 'Enterprise Architecture; ISACA',
  },
  {
    id: 'isc-d9-018',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-data-management',
    topic: 'Data Management',
    subtopic: 'Master Data Management',
    difficulty: 'medium',
    skillLevel: 'Application',
    question: 'Master Data Management (MDM) is critical for financial systems because:',
    options: [
      'It eliminates all IT infrastructure',
      'It ensures a single, authoritative source of truth for key business entities (customers, vendors, employees, chart of accounts, products) across all systems — preventing inconsistencies that could lead to duplicate payments, reporting errors, or compliance failures',
      'It only manages backup data',
      'It replaces the need for any data entry',
    ],
    correctAnswer: 1,
    explanation: 'MDM objectives: create and maintain a single, consistent version of master data across the enterprise. Master data entities: (1) Customer master — name, address, contact, credit terms, (2) Vendor master — name, banking details, tax ID, payment terms, (3) Employee master — personal info, payroll details, cost center, (4) Chart of accounts — GL account structure, (5) Product/material master — descriptions, pricing, units. MDM controls: (1) Single source of truth — changes made in one system propagate to all other systems, (2) Data stewardship — defined owners for each master data domain, (3) Change controls — approval workflow for master data changes (especially vendor bank account changes — high fraud risk), (4) Duplicate detection and prevention, (5) Data quality monitoring — regular audits of master data accuracy. CPA relevance: vendor master data controls are critical for fraud prevention (fictitious vendors, unauthorized bank account changes). Auditors typically test vendor master data changes as part of the AP audit.',
    reference: 'DAMA DMBOK; ISACA',
  },
  {
    id: 'isc-d9-019',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-network',
    topic: 'Network Architecture',
    subtopic: 'Software-Defined Networking',
    difficulty: 'hard',
    skillLevel: 'Analysis',
    question: 'Software-Defined Networking (SDN) separates:',
    options: [
      'Physical cables from wireless connections',
      'The network control plane (routing decisions) from the data plane (packet forwarding) — enabling centralized, programmable network management that can dynamically adjust network configurations and security policies',
      'Servers from storage devices',
      'Applications from databases',
    ],
    correctAnswer: 1,
    explanation: 'SDN architecture: (1) Control plane — centralized controller makes routing decisions (where to send traffic), (2) Data plane — network devices (switches, routers) forward packets based on instructions from the controller, (3) Application plane — applications that communicate with the controller to request network services. Benefits: (1) Centralized management — configure entire network from one point, (2) Programmability — automate network changes, respond to events in real-time, (3) Agility — quickly provision new network segments/policies, (4) Micro-segmentation — granular security policies per workload, (5) Visibility — complete view of network traffic patterns. Security implications: (1) Centralized controller is a high-value target (if compromised, attacker controls entire network), (2) Northbound API security (application-to-controller), (3) Southbound API security (controller-to-devices), (4) Benefits: rapid threat response (can automatically isolate compromised segments). Audit: evaluate controller security, access controls, change management for network policies.',
    reference: 'NIST; ONF (Open Networking Foundation)',
  },
  {
    id: 'isc-d9-020',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-controls',
    topic: 'Application Controls',
    subtopic: 'Document Management Systems',
    difficulty: 'easy',
    skillLevel: 'Remembering and Understanding',
    question: 'An enterprise document management system (DMS) supports internal controls by:',
    options: [
      'Only storing PDF files',
      'Providing version control, access controls, retention management, and audit trails for electronic documents — ensuring that financial documents are properly secured, traceable, and retained according to policy',
      'Eliminating the need for financial documentation',
      'Allowing unlimited anonymous document editing',
    ],
    correctAnswer: 1,
    explanation: 'DMS control features: (1) Version control — tracks all revisions, who made changes, and when; allows rollback to previous versions, (2) Access controls — granular permissions (read, edit, delete, share) based on roles and document classification, (3) Check-in/check-out — prevents concurrent editing conflicts, (4) Audit trail — log of all document access, modifications, downloads, and sharing, (5) Retention management — automated enforcement of retention policies (prevent premature deletion, auto-archive after retention period), (6) Legal hold — prevent destruction of documents relevant to litigation, (7) Search and retrieval — full-text search, metadata-based search for efficient retrieval, (8) Workflow — automated routing for review and approval of documents. CPA relevance: (1) Supports SOX document management requirements, (2) Provides reliable evidence for audit (contract files, board minutes, policy documents), (3) Retention management ensures compliance with record-keeping requirements.',
    reference: 'AIIM; ARMA',
  },
  {
    id: 'isc-d9-021',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-emerging',
    topic: 'Emerging Technologies',
    subtopic: 'Digital Twins',
    difficulty: 'hard',
    skillLevel: 'Analysis',
    question: 'A digital twin in an enterprise context is:',
    options: [
      'A backup copy of a database',
      'A virtual replica of a physical system, process, or product that uses real-time data to simulate, predict, and optimize performance — enabling scenario analysis and proactive decision-making',
      'A duplicate employee badge',
      'A mirror website for disaster recovery',
    ],
    correctAnswer: 1,
    explanation: 'Digital twins: virtual models that mirror physical assets or processes using real-time IoT sensor data. Applications: (1) Manufacturing — simulate production line changes before implementation, (2) Supply chain — model logistics scenarios, predict disruptions, (3) Facilities — building management optimization (energy, HVAC), (4) Financial services — model portfolio risk scenarios, simulate market impacts. Accounting/audit implications: (1) Asset valuation — digital twin data can support fair value measurements of physical assets, (2) Predictive maintenance — affects depreciation and useful life estimates, (3) Operational efficiency — cost reduction quantification, (4) Risk analysis — scenario modeling for financial planning. Challenges: (1) Data accuracy — twin is only as good as its input data, (2) Cybersecurity — IoT sensors and data streams must be secured, (3) Integration — connecting physical systems, IoT platforms, and enterprise systems, (4) Cost — significant investment in sensors, platforms, and expertise.',
    reference: 'Gartner; ISACA Emerging Tech',
  },
  {
    id: 'isc-d9-022',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-audit',
    topic: 'IT Audit Techniques',
    subtopic: 'Data Analytics for Audit',
    difficulty: 'medium',
    skillLevel: 'Application',
    question: 'When using data analytics in an audit, the auditor must first ensure:',
    options: [
      'The analytics tool is the most expensive available',
      'The completeness and accuracy of the data being analyzed — by reconciling extracted data to source system totals, verifying record counts, and assessing data quality before performing analytics',
      'All data is stored in the cloud',
      'The client\'s IT staff performs the analysis',
    ],
    correctAnswer: 1,
    explanation: 'Data analytics audit process: (1) Define objectives — what assertions or risks are being addressed, (2) Data acquisition — obtain data from the client\'s systems (database extracts, API queries, file transfers), (3) Data validation — CRITICAL: verify completeness and accuracy of extracted data: compare record counts to source system, reconcile financial totals to GL, check for truncated or missing fields, verify date ranges match intended period. (4) Data preparation — clean, transform, standardize data for analysis, (5) Analysis — apply analytics (stratification, regression, Benford\'s law, trend analysis, outlier detection, matching/comparison), (6) Evaluation — interpret results, investigate exceptions, assess findings, (7) Documentation — document methodology, data sources, validation procedures, results, and conclusions. Common pitfalls: (1) Analyzing incomplete or inaccurate data (garbage in, garbage out), (2) Drawing incorrect conclusions from correlations, (3) Not understanding the underlying business processes.',
    reference: 'AICPA Data Analytics Guide',
  },
  {
    id: 'isc-d9-023',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-it-governance',
    topic: 'IT Governance',
    subtopic: 'Capacity Planning',
    difficulty: 'medium',
    skillLevel: 'Remembering and Understanding',
    question: 'IT capacity planning ensures that:',
    options: [
      'The organization buys the cheapest servers',
      'IT resources (compute, storage, network bandwidth) are sufficient to meet current and anticipated future demands — preventing performance degradation during peak periods (month-end close, year-end processing) while avoiding unnecessary over-provisioning',
      'All employees have identical computers',
      'New technology is adopted immediately',
    ],
    correctAnswer: 1,
    explanation: 'Capacity planning process: (1) Monitor current usage — CPU, memory, storage, network utilization, transaction volumes, (2) Trend analysis — historical growth patterns, (3) Demand forecasting — project future needs based on business growth, new applications, seasonal peaks, (4) Threshold management — set alerts when utilization approaches limits (e.g., alert at 80% CPU), (5) Scaling — add capacity before thresholds are breached. Types: (1) Vertical scaling (scale up) — add resources to existing servers (more CPU, RAM), (2) Horizontal scaling (scale out) — add more servers/instances, (3) Cloud elasticity — auto-scaling based on demand (particularly useful for peak periods). CPA relevance: (1) Financial systems must handle month-end/year-end processing peaks, (2) Inadequate capacity can delay financial close and reporting, (3) Over-provisioning wastes resources (expense management). Audit consideration: review capacity reports to assess whether financial systems can handle peak processing demands.',
    reference: 'ITIL 4; COBIT DSS',
  },
  {
    id: 'isc-d9-024',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-systems',
    topic: 'Enterprise Systems',
    subtopic: 'Mobile Device Management',
    difficulty: 'medium',
    skillLevel: 'Application',
    question: 'Mobile Device Management (MDM) solutions help organizations:',
    options: [
      'Only track employee locations',
      'Enforce security policies on mobile devices accessing corporate data — including remote wipe capability, encryption requirements, app management, and containerization to separate work and personal data',
      'Prevent employees from using mobile phones',
      'Increase mobile internet speed',
    ],
    correctAnswer: 1,
    explanation: 'MDM/EMM (Enterprise Mobility Management) capabilities: (1) Device management — inventory, configuration, remote lock/wipe for lost/stolen devices, (2) Policy enforcement — password requirements, encryption, jailbreak/root detection, OS version requirements, (3) Application management — approve/block apps, enterprise app store, app configuration, (4) Containerization — separate work data from personal data (BYOD support), work data encrypted in a managed container, (5) Data loss prevention (DLP) — prevent copying/sharing of corporate data to personal apps or external services, (6) Conditional access — device must meet security requirements to access corporate resources (e.g., patch level, no jailbreak), (7) VPN management — automatic VPN configuration for corporate network access. BYOD vs. COPE: (1) BYOD (Bring Your Own Device) — employee-owned; containerization is key, (2) COPE (Corporate-Owned, Personally Enabled) — company-owned; greater control. CPA relevance: mobile access to ERP, email, financial dashboards must be secured.',
    reference: 'NIST SP 800-124; ISACA',
  },
  {
    id: 'isc-d9-025',
    section: 'ISC',
    courseId: 'cpa',
    blueprintArea: 'ISC-I',
    topicId: 'isc-databases',
    topic: 'Database Management',
    subtopic: 'Data Masking and Anonymization',
    difficulty: 'medium',
    skillLevel: 'Application',
    question: 'Data masking is used in non-production environments (development, testing) to:',
    options: [
      'Speed up database queries',
      'Replace sensitive production data (PII, financial data) with realistic but fictional data — allowing developers and testers to work with representative data without exposing actual customer or financial information',
      'Compress database files',
      'Delete unused database tables',
    ],
    correctAnswer: 1,
    explanation: 'Data masking techniques: (1) Static masking — permanently replaces sensitive data in a copy of the database used for development/testing, (2) Dynamic masking — real-time masking at query time; production data remains unchanged but different users see masked versions based on their privileges, (3) Tokenization — replaces sensitive data with non-reversible tokens (different from encryption — tokens have no mathematical relationship to original data), (4) Pseudonymization — replaces identifiers with pseudonyms; can be reversed with the mapping table. Why it matters: (1) Development/test environments often have weaker access controls than production, (2) Regulatory requirements (GDPR, CCPA) restrict use of real personal data for testing, (3) Reduces risk of data breaches from non-production environments. CPA relevance: (1) Non-production environments with real financial data pose fraud and privacy risks, (2) Auditors should verify that sensitive data is masked in all non-production environments, (3) Dynamic masking supports least-privilege data access.',
    reference: 'NIST Privacy Framework; GDPR Art. 25',
  },
];
